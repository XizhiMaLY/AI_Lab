{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入程序所需要的包\n",
    "\n",
    "# PyTorch需要的包\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "import torch.utils.data as DataSet\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary  # 需要预先下载，在终端输入 pip install torchsummary\n",
    "\n",
    "# 计算需要的包\n",
    "import string\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变', '春', '逐', '鸟', '声', '开', '初', '风', '飘', '带', '柳', '晚', '雪', '间', '花', '梅', '碧', '林', '青', '旧', '竹', '绿', '沼', '翠', '新', '苔', '芝', '田', '初', '雁', '去', '绮', '树', '巧', '莺', '来']\n"
     ]
    }
   ],
   "source": [
    "# 读入并展示数据\n",
    "f = open('./poems_clean.txt', \"r\", encoding='utf-8')\n",
    "poems = []\n",
    "for line in f.readlines():\n",
    "    title, poem = line.split(':')\n",
    "    poem = poem.replace(' ', '')\n",
    "    poem = poem.replace('\\n', '')\n",
    "    if len(poem) > 0 :\n",
    "        poems.append(list(poem))\n",
    "    \n",
    "print(poems[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('寒', 1),\n",
       " ('随', 2),\n",
       " ('穷', 3),\n",
       " ('律', 4),\n",
       " ('变', 5),\n",
       " ('春', 6),\n",
       " ('逐', 7),\n",
       " ('鸟', 8),\n",
       " ('声', 9),\n",
       " ('开', 10)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建字符编码字典\n",
    "word2idx = {}\n",
    "i = 1\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word2idx.get(word) == None:\n",
    "            word2idx[word] = i\n",
    "            i += 1\n",
    "            \n",
    "list(word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5545"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始诗歌\n",
      "['春', '眠', '不', '觉', '晓', '处', '处', '闻', '啼', '鸟', '夜', '来', '风', '雨', '声', '花', '落', '知', '多', '少']\n",
      "\n",
      " 编码后的结果\n",
      "[6, 2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 1941]\n"
     ]
    }
   ],
   "source": [
    "# 对诗歌进行编码，从原始数据到矩阵\n",
    "poems_digit = []\n",
    "for poem in poems:\n",
    "    poem_digit = []\n",
    "    for word in poem:\n",
    "        poem_digit.append(word2idx[word])\n",
    "    poems_digit.append(poem_digit)\n",
    "    \n",
    "print(\"原始诗歌\")\n",
    "print(poems[3829])\n",
    "print(\"\\n 编码后的结果\")\n",
    "print(poems_digit[3829][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始诗歌\n",
      "['春', '眠', '不', '觉', '晓', '处', '处', '闻', '啼', '鸟', '夜', '来', '风', '雨', '声', '花', '落', '知', '多', '少']\n",
      "变量X\n",
      "[6, 2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量Y\n",
      "[2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 1941, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 拆分X、Y变量并处理长短不一问题\n",
    "# 设置诗歌最大长度为50个字符\n",
    "maxlen = 50\n",
    "X = []\n",
    "Y = []\n",
    "for poem_digit in poems_digit:\n",
    "    y=poem_digit[1:]+[0]*(maxlen - len(poem_digit)) # 此处修改y \n",
    "    Y.append(y)\n",
    "    # 将最后一个字符之前的部分作为X，并补齐字符\n",
    "    x = poem_digit[:-1] + [0]*(maxlen - len(poem_digit))\n",
    "    X.append(x)\n",
    "    \n",
    "print(\"原始诗歌\")\n",
    "print(poems[3829])\n",
    "print(\"变量X\")\n",
    "print(X[3829])\n",
    "print(\"变量Y\")\n",
    "print(Y[3829])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 将所有数据的顺序打乱重排\n",
    "idx = np.random.permutation(range(len(X)))\n",
    "X = [X[i] for i in idx]\n",
    "Y = [Y[i] for i in idx]\n",
    "\n",
    "# 切分出1/5的数据放入校验集    \n",
    "validX = X[:len(X) // 5]\n",
    "trainX = X[len(X) // 5:]\n",
    "validY = Y[:len(Y) // 5]\n",
    "trainY = Y[len(Y) // 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "将数据转化为dataset，并用dataloader来加载数据。dataloader是PyTorch开发采用的一套管理数据的方法。通常数据储存在dataset中，而对数据的调用则由\n",
    "dataloader完成。同时，在预处理时，系统已经自动将数据打包成batch，每次调用都取出一批。从dataloader中输出的每一个元素都是一个(x,y)元组，x为输\n",
    "入张量，y为标签。x和y的第一个维度都是batch_size大小。\n",
    "'''\n",
    "\n",
    "# 一批包含64个数据记录。这个数字越大，系统训练时，每一个周期要处理的数据就越多、处理就越快，但总数据量会减少。\n",
    "batch_size = 64\n",
    "# 形成训练集\n",
    "train_ds = DataSet.TensorDataset(torch.IntTensor(np.array(trainX, dtype=int)), torch.IntTensor(np.array(trainY, dtype=int)))\n",
    "# 形成数据加载器\n",
    "train_loader = DataSet.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# 校验数据\n",
    "valid_ds = DataSet.TensorDataset(torch.IntTensor(np.array(validX, dtype=int)), torch.IntTensor(np.array(validY, dtype=int)))\n",
    "valid_loader = DataSet.DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37082bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_py(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(RNN_py, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # Initialize parameters that can be updated\n",
    "        self.weight_ih = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size, embedding_size)) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.bias_ih = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.weight_hh = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size, hidden_size)) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.bias_hh = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        h_t_minus_1 = h_0\n",
    "        h_t = h_0\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            for layer in range(self.num_layers):\n",
    "                h_t[layer] = torch.tanh(\n",
    "                    x[t] @ self.weight_ih[layer].T\n",
    "                    + self.bias_ih[layer]\n",
    "                    + h_t_minus_1[layer] @ self.weight_hh[layer].T\n",
    "                    + self.bias_hh[layer]\n",
    "                )\n",
    "            output.append(h_t[-1])\n",
    "            h_t_minus_1 = h_t\n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        return output, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "实现一个简单的RNN，其构架主要包含3层：输入层，一层隐含层和输出层\n",
    "先是embedding层，将词表转化成embedding\n",
    "后面是hidden size\n",
    "最后接一个输出为词表维度的线性层，接crossentropy求loss\n",
    "'''\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, output_size, word_num, embedding_size, hidden_size, num_layers=5):\n",
    "        # 定义\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # 一个embedding层\n",
    "        self.embedding = nn.Embedding(word_num, embedding_size)\n",
    "        \n",
    "        # PyTorch的RNN层，batch_first标识可以让输入的张量的第一个维度表示batch指标\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.rnn_py = RNN_py(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # 输出的全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "                \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # 运算过程\n",
    "        # 先进行embedding层的计算\n",
    "        x = self.embedding(x)\n",
    "        # 从输入到隐含层的计算\n",
    "        # x的尺寸为：batch_size，num_step，hidden_size\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        # output的尺寸为：batch_size，maxlen-1, hidden_size\n",
    "        # 最后一层全连接网络 此处返回每个时间步的数值\n",
    "        output = self.fc(output)\n",
    "        output = output.view(-1,output.shape[-1])#为便于后续处理，此处进行展平\n",
    "        # output的尺寸为：batch_size*(maxlen-1)，output_size\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        # 对隐含单元初始化\n",
    "        # 尺寸是layer_size，batch_size，hidden_size\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(5546, 64)\n",
      "  (rnn): RNN(64, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=5546, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 获取文本数据集中包含的字符数量\n",
    "vocab_size = len(word2idx.keys()) + 1\n",
    "\n",
    "# 给定超参数\n",
    "lr = 1e-3\n",
    "epochs = 3\n",
    "\n",
    "# 生成一个简单的RNN，输入size为49（50-1），输出size为vocab_size（字符总数）\n",
    "rnn = SimpleRNN(output_size=vocab_size, word_num=vocab_size, embedding_size=64, hidden_size=128)\n",
    "rnn = rnn.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr) #Adam优化算法\n",
    "# 查看模型具体信息\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "计算预测错误率的函数，pre是模型给出的一组预测结果（batch_size行、num_classes列的矩阵），label是正确标签\n",
    "'''\n",
    "\n",
    "def accuracy(pre, label):\n",
    "    # 得到每一行（每一个样本）输出值最大元素的下标\n",
    "    pre = torch.max(pre.data, 1)[1]\n",
    "    # 将下标与label比较，计算正确的数量\n",
    "    rights = pre.eq(label.data).sum()\n",
    "    # 计算正确预测所占百分比\n",
    "    acc = rights.data/len(label)\n",
    "    return acc.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def validate(model, val_loader):\n",
    "    # 在校验集上运行一遍并计算损失和准确率\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    for batch, data in enumerate(val_loader):\n",
    "        init_hidden = model.initHidden(len(data[0]))\n",
    "        init_hidden = init_hidden.cuda()\n",
    "        x, y = Variable(data[0]), Variable(data[1])\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        outputs, hidden = model(x, init_hidden)\n",
    "        y = y.long()\n",
    "        y = y.view(y.shape[0]*y.shape[1]) #此处修改：展平，对应x的维度\n",
    "        loss = criterion(outputs, y)\n",
    "        val_loss += loss.data.cpu().numpy()  \n",
    "        val_acc += accuracy(outputs, y)\n",
    "    val_loss /= len(val_loader)  # 计算平均损失\n",
    "    val_acc /= len(val_loader)  # 计算平均准确率\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练结果\n",
    "def print_log(epoch, train_time, train_loss, train_acc, val_loss, val_acc, epochs=10):\n",
    "    print(f\"Epoch [{epoch}/{epochs}], time: {train_time:.2f}s, loss: {train_loss:.4f}, acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义主函数：模型训练\n",
    "def train(model,optimizer, train_loader, val_loader, epochs=1):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        start = time.time()  # 记录本epoch开始时间\n",
    "        for batch, data in enumerate(train_loader):\n",
    "            # batch为数字，表示已经进行了几个batch\n",
    "            # data为一个二元组，存储了一个样本的输入和标签\n",
    "            model.train() # 标志当前RNN处于训练阶段\n",
    "            init_hidden = model.initHidden(len(data[0])) # 初始化隐含层单元\n",
    "            init_hidden = init_hidden.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = Variable(data[0]), Variable(data[1]) # 从数据中提取输入和输出对\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, hidden = model(x, init_hidden) # 输入RNN，产生输出\n",
    "            y = y.long()\n",
    "            y = y.view(y.shape[0]*y.shape[1]) #此处修改：展平，对应x的维度\n",
    "            loss = criterion(outputs, y) # 带入损失函数并产生loss\n",
    "            train_loss += loss.data.cpu().numpy()  # 记录loss\n",
    "            train_acc += accuracy(outputs, y) # 记录acc\n",
    "            loss.backward() # 反向传播\n",
    "            optimizer.step() # 梯度更新\n",
    "        \n",
    "        end = time.time()  # 记录本epoch结束时间\n",
    "        train_time = end - start  # 计算本epoch的训练耗时 \n",
    "        train_loss /= len(train_loader)  # 计算平均损失\n",
    "        train_acc /= len(train_loader)  # 计算平均准确率             \n",
    "        val_loss, val_acc = validate(model, val_loader)  # 计算测试集上的损失函数和准确率\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc) \n",
    "        print_log(epoch + 1, train_time, train_loss, train_acc, val_loss, val_acc, epochs=epochs)  # 打印训练结果\n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], time: 1.88s, loss: 4.8394, acc: 0.3466, val_loss: 4.6052, val_acc: 0.3510\n",
      "Epoch [2/3], time: 1.70s, loss: 4.4951, acc: 0.3609, val_loss: 4.4768, val_acc: 0.3612\n",
      "Epoch [3/3], time: 1.93s, loss: 4.3563, acc: 0.3698, val_loss: 4.3684, val_acc: 0.3692\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "history = train(rnn, optimizer, train_loader, valid_loader, epochs=epochs)  # 实施训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "162ddfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rnn(torch.tensor(X[3829]).cuda().unsqueeze(-1), rnn.initHidden(len(X[3829])).cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9873ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 5546])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f11cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  12,  533,   48,  113,  309,   67,  788,  445,   48,  213,    0,    0,\n",
      "          34,   59,   48, 2435,    0,  113,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming 'output' is your tensor with shape torch.Size([49, 5546])\n",
    "# output = torch.randn(49, 5546)  # Example tensor\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(output, dim=1)\n",
    "\n",
    "# Get the token IDs by finding the index of the maximum value in each row\n",
    "token_ids = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56a02a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 49 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m current_word \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m word2idx\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39m==\u001b[39m token_ids\u001b[39m.\u001b[39mitem()][\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m current_word\n",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m current_word \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m word2idx\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39m==\u001b[39m token_ids\u001b[39m.\u001b[39;49mitem()][\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m current_word\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 49 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "current_word = [k for k, v in word2idx.items() if v == token_ids.item()][0]\n",
    "current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a15731dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c5e30cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m idx2word[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "idx2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee6f2257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['风', '年', '日', '山', '离', '知', '规', '有', '日', '云', '0', '0', '去', '园', '日', '羊', '0', '山', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "# Create a reverse dictionary\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "idx2word[0] = '0'\n",
    "# Assuming 'token_ids' is the tensor from the previous step\n",
    "# token_ids = torch.tensor([1, 2, 1])  # Example token IDs\n",
    "\n",
    "# Get the words corresponding to the token IDs\n",
    "words_from_ids = [idx2word[idx.item()] for idx in token_ids]\n",
    "print(words_from_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8d9438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['仙', '娥', '今', '下', '嫁', '骄', '子', '自', '同', '和', '剑', '戟', '归', '田', '尽', '牛', '羊', '绕', '塞', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print([idx2word[idx] for idx in X[3829]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a3f7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['娥', '今', '下', '嫁', '骄', '子', '自', '同', '和', '剑', '戟', '归', '田', '尽', '牛', '羊', '绕', '塞', '多', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print([idx2word[idx] for idx in Y[3829]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用RNN写藏头诗\n",
    "# 初始化藏头诗字符串\n",
    "poem_incomplete = '深****度****学****习****'\n",
    "poem_index = [] # 用于记录诗歌创作过程中字符和整数的对应关系\n",
    "poem_text = '' # 记录诗歌的创作过程，循环结束后应是一首完整的诗\n",
    "\n",
    "init_hidden = rnn.initHidden(1)\n",
    "init_hidden = init_hidden.cuda()\n",
    "for i in range(len(poem_incomplete)):\n",
    "    # 对poem_incomplete的每个字符做循环\n",
    "    current_word = poem_incomplete[i]\n",
    "    if current_word != '*':\n",
    "        # 若当前的字符不是\"*\"，使用word2idx字典将其变为一个整数\n",
    "        index = word2idx[current_word]\n",
    "    else:\n",
    "        # 若当前的字符是\"*\"，需要用RNN模型对其进行预测\n",
    "        x = poem_index + [0]*(maxlen -1 - len(poem_index)) # 将当前字符与之前的字符拼接形成新的输入序列\n",
    "        x = torch.IntTensor(np.array([x], dtype=int))\n",
    "        x = Variable(x)\n",
    "        x = x.cuda()\n",
    "        pre, hidden = rnn(x, init_hidden) \n",
    "        init_hidden = hidden.data # 更新隐藏状态\n",
    "        crt_pre = pre[i-1].cpu() # 获取第i个时间步的输出，对应位置i-1\n",
    "        index = torch.argmax(crt_pre) # 提取最大概率的字符所在的位置，记录其编号\n",
    "        current_word = [k for k, v in word2idx.items() if v == index][0] # 提取上述编号所对应的字符\n",
    "    poem_index.append(index) \n",
    "    poem_text = poem_text + current_word # 将current_word加到poem_text中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深山下石溪\n",
      "度月明月明\n",
      "学得意何处\n",
      "习家家女泪\n"
     ]
    }
   ],
   "source": [
    "print(poem_text[0:5])\n",
    "print(poem_text[5:10])\n",
    "print(poem_text[10:15])\n",
    "print(poem_text[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
