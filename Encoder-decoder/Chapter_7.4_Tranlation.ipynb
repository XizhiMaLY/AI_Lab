{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7844053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关模块\n",
    "import time\n",
    "import string\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as DataSet\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c3a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/agent_mxz/miniconda3/envs/cops3/lib/python3.10/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp1g3960ad' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.469 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'I', 'were', 'you,', \"I'd\", 'want', 'to', 'know', 'what', 'Tom', 'is', 'doing', 'right', 'now']\n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n"
     ]
    }
   ],
   "source": [
    "### 定义读取语料方法\n",
    "def read_corpus(path):\n",
    "    English = []\n",
    "    Chinese = []\n",
    "\n",
    "    f = open(path, 'r', encoding = 'utf-8')\n",
    "    for line in f.readlines():\n",
    "        eng, chs = line.strip().split('\\t')\n",
    "\n",
    "        eng = eng[:-1]\n",
    "        eng = eng.split(' ')\n",
    "        English.append(eng)\n",
    "\n",
    "        chs = jieba.lcut(chs)\n",
    "        chs = ['B'] + chs\n",
    "        Chinese.append(chs)\n",
    "    return English, Chinese\n",
    "\n",
    "English, Chinese = read_corpus('./cmn.txt')\n",
    "print(English[20000])\n",
    "print(Chinese[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8054d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典大小 13683\n",
      "英文字典大小 7814\n"
     ]
    }
   ],
   "source": [
    "### 定义中英文字典编码方法\n",
    "def lang_encode(language):\n",
    "    lang2idx = {}\n",
    "    i = 1\n",
    "    for chs in language:\n",
    "        for c in chs:\n",
    "            if lang2idx.get(c) == None:\n",
    "                lang2idx[c] = i\n",
    "                i += 1\n",
    "    return lang2idx\n",
    "\n",
    "chs2idx = lang_encode(Chinese)\n",
    "eng2idx = lang_encode(English)\n",
    "chs_vocab_size = len(chs2idx.keys()) + 1\n",
    "eng_vocab_size = len(eng2idx.keys()) + 1\n",
    "\n",
    "print('中文字典大小', chs_vocab_size)\n",
    "print('英文字典大小', eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f87f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始中文: \n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n",
      "\n",
      " 中文编码后的结果: \n",
      "[1, 917, 12, 35, 5, 79, 12, 16, 257, 36, 73, 202, 429, 496, 129, 299, 3]\n",
      "原始英文: \n",
      "['If', 'I', 'were', 'you,', \"I'd\", 'want', 'to', 'know', 'what', 'Tom', 'is', 'doing', 'right', 'now']\n",
      "\n",
      " 英文编码后的结果: \n",
      "[1057, 5, 734, 1013, 673, 302, 484, 66, 993, 29, 199, 1017, 142, 112]\n"
     ]
    }
   ],
   "source": [
    "### 定义文本编码方法\n",
    "def text_encode(lang2idx, language):\n",
    "    text_digit = []\n",
    "    for txt in language:\n",
    "        t_digit = []\n",
    "        for t in txt:\n",
    "            t_digit.append(lang2idx[t])\n",
    "        text_digit.append(t_digit)\n",
    "    return text_digit\n",
    "\n",
    "chs_digit = text_encode(chs2idx, Chinese)\n",
    "eng_digit = text_encode(eng2idx, English)\n",
    "print(\"原始中文: \")\n",
    "print(Chinese[20000])\n",
    "print(\"\\n 中文编码后的结果: \")\n",
    "print(chs_digit[20000][:])\n",
    "print(\"原始英文: \")\n",
    "print(English[20000])\n",
    "print(\"\\n 英文编码后的结果: \")\n",
    "print(eng_digit[20000][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae389bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始中文: \n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n",
      "变量X_chs: \n",
      "[1, 917, 12, 35, 5, 79, 12, 16, 257, 36, 73, 202, 429, 496, 129, 299, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量X_eng: \n",
      "[1057, 5, 734, 1013, 673, 302, 484, 66, 993, 29, 199, 1017, 142, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量Y: \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "### 定义生成训练输入输出序列函数\n",
    "def generate_XY(chs_digit, eng_digit, max_len):\n",
    "    X = []\n",
    "    Y = []\n",
    "    i = -1\n",
    "    for c_digit in chs_digit:\n",
    "        i += 1\n",
    "        Y.append(c_digit[-1])\n",
    "        # 将最后一个字符之前的部分作为X，并补齐字符\n",
    "        x1 = c_digit[:-1] + [0]*(max_len - len(c_digit))\n",
    "        x2 = eng_digit[i] + [0]*(max_len - len(eng_digit[i]))\n",
    "        X.append(x1 + x2)    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_XY(chs_digit, eng_digit, max_len=40)\n",
    "print(\"原始中文: \")\n",
    "print(Chinese[20000])\n",
    "print(\"变量X_chs: \")\n",
    "print(X[20000][0:39])\n",
    "print(\"变量X_eng: \")\n",
    "print(X[20000][-40:])\n",
    "print(\"变量Y: \")\n",
    "print(Y[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07728ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "# 将所有数据的顺序打乱重排\n",
    "idx = np.random.permutation(range(len(X)))\n",
    "X = [X[i] for i in idx]\n",
    "Y = [Y[i] for i in idx]\n",
    "\n",
    "# 切分出1/5的数据作为验证集  \n",
    "validX = X[: len(X) // 5]\n",
    "trainX = X[len(X) // 5 :]\n",
    "validY = Y[: len(Y) // 5]\n",
    "trainY = Y[len(Y) // 5 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2400a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定batch size\n",
    "batch_size = 64\n",
    "# 创建Tensor形式的训练集\n",
    "train_ds = DataSet.TensorDataset(torch.IntTensor(np.array(trainX, dtype=int)), \n",
    "                                 torch.IntTensor(np.array(trainY, dtype=int)))\n",
    "# 形成训练数据加载器\n",
    "train_loader = DataSet.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# 创建Tensor形式的验证集\n",
    "valid_ds = DataSet.TensorDataset(torch.IntTensor(np.array(validX, dtype=int)), \n",
    "                                 torch.IntTensor(np.array(validY, dtype=int)))\n",
    "# 形成验证数据加载器\n",
    "valid_loader = DataSet.DataLoader(valid_ds, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9805d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义机器翻译网络结构\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, eng_vocab_size, chs_vocab_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(Translator, self).__init__()        \n",
    "        # Pytorch的LSTM层，batch_first标识可以让输入的张量的第一个维度表示batch指标\n",
    "        self.encoder_embedding = nn.Embedding(eng_vocab_size, embedding_size)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder_embedding = nn.Embedding(chs_vocab_size, embedding_size)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, chs_vocab_size)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.eng_vocab_size = eng_vocab_size\n",
    "        self.chs_vocab_size = chs_vocab_size\n",
    "    \n",
    "    # 定义前向计算流程\n",
    "    def forward(self, chs, encoder_state):        \n",
    "        # 输入为代表音乐家的one-hot向量，转为具有一定意义的特征向量\n",
    "        x = self.decoder_embedding(chs)\n",
    "        # 读入隐含层的初始信息\n",
    "        hh = encoder_state\n",
    "        # 从输入到隐含层的计算\n",
    "        # x的尺寸为：batch_size，num_step，hidden_size\n",
    "        output, hidden = self.decoder_lstm(x, hh)\n",
    "        # 从output中去除最后一个时间步的数值（output中包含了所有时间步的结果）\n",
    "        output = output[:, -1, ...]\n",
    "        # output的尺寸为：batch_size，hidden_size\n",
    "        # 最后一层全连接网络\n",
    "        output = self.fc(output)\n",
    "        # output的尺寸为：batch_size，output_size\n",
    "        return output\n",
    "    \n",
    "    # 定义隐含单元初始化方法\n",
    "    def initHidden(self, eng, batch_size):       \n",
    "        x = self.encoder_embedding(eng).cuda()    \n",
    "        # 初始化的隐藏元和记忆元,通常它们的维度是一样的\n",
    "        h1 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).cuda()\n",
    "        c1 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).cuda()\n",
    "        #这里我们要对后面的LSTM模型的隐藏状态进行条件初始化\n",
    "        _, encoder_state = self.encoder_lstm(x, (h1, c1))   \n",
    "        return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abc3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder_embedding): Embedding(7814, 64)\n",
      "  (encoder_lstm): LSTM(64, 128, batch_first=True)\n",
      "  (decoder_embedding): Embedding(13683, 64)\n",
      "  (decoder_lstm): LSTM(64, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=13683, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#给定超参数\n",
    "lr = 1e-2\n",
    "epochs = 200\n",
    "# 创建机器翻译模型实例\n",
    "translator = Translator(eng_vocab_size=eng_vocab_size, chs_vocab_size=chs_vocab_size, embedding_size=64, hidden_size=128)\n",
    "# 转为GPU下的模型\n",
    "translator = translator.cuda()\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "# sgd优化算法\n",
    "optimizer = torch.optim.SGD(translator.parameters(), lr=lr) \n",
    "#查看模型具体信息\n",
    "print(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf3f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义预测准确率的函数\n",
    "def accuracy(pre, label):\n",
    "    # 得到每一行（每一个样本）输出值最大元素的下标\n",
    "    pre = torch.max(pre.data, 1)[1]\n",
    "    # 将下标与label比较，计算正确的数量\n",
    "    rights = pre.eq(label.data).sum()\n",
    "    # 计算正确预测所占百分比\n",
    "    acc = rights.data / len(label)\n",
    "    return acc.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a73f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义一个tensor分割函数\n",
    "def split_chs_eng(x):\n",
    "    x = x.tolist()\n",
    "    x1 = [x[i][0:40] for i in range(len(x))]\n",
    "    x2 = [x[i][-40:] for i in range(len(x))]\n",
    "    x1 = torch.IntTensor(np.array(x1, dtype=int))\n",
    "    x2 = torch.IntTensor(np.array(x2, dtype=int))\n",
    "    return Variable(x1).cuda(), Variable(x2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e51393",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义训练过程打印函数\n",
    "def print_log(epoch, train_time, train_loss, train_acc, val_loss, val_acc, epochs=10):\n",
    "    print(f\"Epoch [{epoch}/{epochs}], time: {train_time:.2f}s, loss: {train_loss:.4f}, acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe390c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型验证过程\n",
    "def validate(model, val_loader):\n",
    "    # 在验证集上运行一遍并计算损失和准确率\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    for batch, data in enumerate(val_loader):\n",
    "        x, y = Variable(data[0]), Variable(data[1])\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        chs, eng = split_chs_eng(x)\n",
    "        encoder_state = model.initHidden(eng, len(data[0]))\n",
    "        outputs = model(chs, encoder_state)\n",
    "        y = y.long()\n",
    "        loss = criterion(outputs, y)\n",
    "        val_loss += loss.data.cpu().numpy()  \n",
    "        val_acc += accuracy(outputs, y)\n",
    "    # 计算平均损失\n",
    "    val_loss /= len(val_loader)  \n",
    "    # 计算平均准确率\n",
    "    val_acc /= len(val_loader)  \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c68924",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型训练函数\n",
    "def train(model, optimizer, train_loader, val_loader, epochs=50):  \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        # 记录当前epoch开始时间\n",
    "        start = time.time()  \n",
    "        for batch, data in enumerate(train_loader):\n",
    "            # batch为数字，表示已经进行了几个batch\n",
    "            # data为一个二元组，存储了一个样本的输入和标签\n",
    "            model.train() \n",
    "            x, y = Variable(data[0]), Variable(data[1])\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            chs, eng = split_chs_eng(x)\n",
    "            encoder_state = model.initHidden(eng, len(data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(chs, encoder_state)\n",
    "            y = y.long()\n",
    "            # 计算当前损失\n",
    "            loss = criterion(outputs, y) \n",
    "            train_loss += loss.data.cpu().numpy()  \n",
    "            train_acc += accuracy(outputs, y) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "        # 记录当前epoch结束时间\n",
    "        end = time.time()  \n",
    "        # 计算当前epoch的训练耗时 \n",
    "        train_time = end - start  \n",
    "        # 计算平均损失\n",
    "        train_loss /= len(train_loader) \n",
    "        # 计算平均准确率 \n",
    "        train_acc /= len(train_loader) \n",
    "        # 计算验证集上的损失函数和准确率\n",
    "        val_loss, val_acc = validate(model, val_loader)  \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc) \n",
    "        print_log(epoch + 1, train_time, train_loss, train_acc, val_loss, val_acc, epochs=epochs)  \n",
    "        \n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0b788d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], time: 2.63s, loss: 2.9832, acc: 0.8131, val_loss: 0.8309, val_acc: 0.8563\n",
      "Epoch [2/200], time: 2.65s, loss: 0.7939, acc: 0.8412, val_loss: 0.6455, val_acc: 0.8560\n",
      "Epoch [3/200], time: 2.63s, loss: 0.6858, acc: 0.8412, val_loss: 0.5743, val_acc: 0.8560\n",
      "Epoch [4/200], time: 2.63s, loss: 0.6040, acc: 0.8424, val_loss: 0.5106, val_acc: 0.8564\n",
      "Epoch [5/200], time: 2.73s, loss: 0.5474, acc: 0.8424, val_loss: 0.4717, val_acc: 0.8561\n",
      "Epoch [6/200], time: 2.78s, loss: 0.5105, acc: 0.8520, val_loss: 0.4438, val_acc: 0.8743\n",
      "Epoch [7/200], time: 2.69s, loss: 0.4852, acc: 0.8722, val_loss: 0.4209, val_acc: 0.8912\n",
      "Epoch [8/200], time: 2.66s, loss: 0.4573, acc: 0.8840, val_loss: 0.3996, val_acc: 0.9003\n",
      "Epoch [9/200], time: 2.74s, loss: 0.4372, acc: 0.8961, val_loss: 0.3853, val_acc: 0.9117\n",
      "Epoch [10/200], time: 2.62s, loss: 0.4210, acc: 0.8994, val_loss: 0.3711, val_acc: 0.9153\n",
      "Epoch [11/200], time: 2.64s, loss: 0.4069, acc: 0.9026, val_loss: 0.3608, val_acc: 0.9170\n",
      "Epoch [12/200], time: 3.26s, loss: 0.3962, acc: 0.9064, val_loss: 0.3528, val_acc: 0.9201\n",
      "Epoch [13/200], time: 3.98s, loss: 0.3866, acc: 0.9133, val_loss: 0.3476, val_acc: 0.9224\n",
      "Epoch [14/200], time: 3.87s, loss: 0.3792, acc: 0.9188, val_loss: 0.3429, val_acc: 0.9291\n",
      "Epoch [15/200], time: 3.38s, loss: 0.3894, acc: 0.9199, val_loss: 0.3436, val_acc: 0.9237\n",
      "Epoch [16/200], time: 4.24s, loss: 0.3669, acc: 0.9238, val_loss: 0.3311, val_acc: 0.9317\n",
      "Epoch [17/200], time: 4.15s, loss: 0.3613, acc: 0.9263, val_loss: 0.3284, val_acc: 0.9316\n",
      "Epoch [18/200], time: 4.15s, loss: 0.3604, acc: 0.9267, val_loss: 0.3496, val_acc: 0.9154\n",
      "Epoch [19/200], time: 4.14s, loss: 0.3523, acc: 0.9289, val_loss: 0.3233, val_acc: 0.9329\n",
      "Epoch [20/200], time: 4.28s, loss: 0.3509, acc: 0.9289, val_loss: 0.3185, val_acc: 0.9351\n",
      "Epoch [21/200], time: 4.15s, loss: 0.3442, acc: 0.9299, val_loss: 0.3148, val_acc: 0.9344\n",
      "Epoch [22/200], time: 4.25s, loss: 0.3412, acc: 0.9306, val_loss: 0.3108, val_acc: 0.9339\n",
      "Epoch [23/200], time: 4.12s, loss: 0.3369, acc: 0.9311, val_loss: 0.3088, val_acc: 0.9362\n",
      "Epoch [24/200], time: 4.13s, loss: 0.3336, acc: 0.9316, val_loss: 0.3076, val_acc: 0.9363\n",
      "Epoch [25/200], time: 3.39s, loss: 0.3340, acc: 0.9307, val_loss: 0.3062, val_acc: 0.9338\n",
      "Epoch [26/200], time: 3.64s, loss: 0.3277, acc: 0.9323, val_loss: 0.3020, val_acc: 0.9358\n",
      "Epoch [27/200], time: 4.32s, loss: 0.3249, acc: 0.9336, val_loss: 0.3000, val_acc: 0.9349\n",
      "Epoch [28/200], time: 4.20s, loss: 0.3295, acc: 0.9335, val_loss: 0.3027, val_acc: 0.9398\n",
      "Epoch [29/200], time: 4.16s, loss: 0.3197, acc: 0.9357, val_loss: 0.2973, val_acc: 0.9419\n",
      "Epoch [30/200], time: 4.02s, loss: 0.3173, acc: 0.9365, val_loss: 0.2951, val_acc: 0.9428\n",
      "Epoch [31/200], time: 4.32s, loss: 0.3156, acc: 0.9365, val_loss: 0.2978, val_acc: 0.9394\n",
      "Epoch [32/200], time: 4.23s, loss: 0.3128, acc: 0.9370, val_loss: 0.2919, val_acc: 0.9428\n",
      "Epoch [33/200], time: 4.12s, loss: 0.3111, acc: 0.9372, val_loss: 0.2901, val_acc: 0.9432\n",
      "Epoch [34/200], time: 4.16s, loss: 0.3087, acc: 0.9371, val_loss: 0.2905, val_acc: 0.9417\n",
      "Epoch [35/200], time: 4.22s, loss: 0.3068, acc: 0.9370, val_loss: 0.2887, val_acc: 0.9433\n",
      "Epoch [36/200], time: 4.01s, loss: 0.3050, acc: 0.9371, val_loss: 0.2870, val_acc: 0.9450\n",
      "Epoch [37/200], time: 4.07s, loss: 0.3038, acc: 0.9380, val_loss: 0.2884, val_acc: 0.9416\n",
      "Epoch [38/200], time: 4.17s, loss: 0.3021, acc: 0.9380, val_loss: 0.2855, val_acc: 0.9424\n",
      "Epoch [39/200], time: 4.26s, loss: 0.3009, acc: 0.9384, val_loss: 0.2835, val_acc: 0.9433\n",
      "Epoch [40/200], time: 4.14s, loss: 0.3029, acc: 0.9372, val_loss: 0.3187, val_acc: 0.9337\n",
      "Epoch [41/200], time: 4.03s, loss: 0.2985, acc: 0.9389, val_loss: 0.2810, val_acc: 0.9443\n",
      "Epoch [42/200], time: 4.26s, loss: 0.2967, acc: 0.9388, val_loss: 0.2824, val_acc: 0.9425\n",
      "Epoch [43/200], time: 4.22s, loss: 0.2954, acc: 0.9390, val_loss: 0.2811, val_acc: 0.9466\n",
      "Epoch [44/200], time: 4.29s, loss: 0.2944, acc: 0.9393, val_loss: 0.2779, val_acc: 0.9435\n",
      "Epoch [45/200], time: 4.16s, loss: 0.2931, acc: 0.9393, val_loss: 0.2782, val_acc: 0.9435\n",
      "Epoch [46/200], time: 3.94s, loss: 0.2922, acc: 0.9396, val_loss: 0.2775, val_acc: 0.9442\n",
      "Epoch [47/200], time: 3.76s, loss: 0.2910, acc: 0.9399, val_loss: 0.2770, val_acc: 0.9454\n",
      "Epoch [48/200], time: 4.33s, loss: 0.2903, acc: 0.9402, val_loss: 0.2771, val_acc: 0.9460\n",
      "Epoch [49/200], time: 4.22s, loss: 0.2919, acc: 0.9396, val_loss: 0.2773, val_acc: 0.9466\n",
      "Epoch [50/200], time: 4.35s, loss: 0.2882, acc: 0.9405, val_loss: 0.2733, val_acc: 0.9457\n",
      "Epoch [51/200], time: 4.07s, loss: 0.2896, acc: 0.9396, val_loss: 0.2757, val_acc: 0.9439\n",
      "Epoch [52/200], time: 4.15s, loss: 0.2864, acc: 0.9411, val_loss: 0.2724, val_acc: 0.9451\n",
      "Epoch [53/200], time: 4.07s, loss: 0.2853, acc: 0.9410, val_loss: 0.2717, val_acc: 0.9451\n",
      "Epoch [54/200], time: 4.37s, loss: 0.2845, acc: 0.9412, val_loss: 0.2704, val_acc: 0.9485\n",
      "Epoch [55/200], time: 4.26s, loss: 0.2892, acc: 0.9400, val_loss: 0.2859, val_acc: 0.9421\n",
      "Epoch [56/200], time: 3.84s, loss: 0.2833, acc: 0.9415, val_loss: 0.2692, val_acc: 0.9483\n",
      "Epoch [57/200], time: 3.52s, loss: 0.2997, acc: 0.9403, val_loss: 0.2842, val_acc: 0.9420\n",
      "Epoch [58/200], time: 4.18s, loss: 0.2817, acc: 0.9416, val_loss: 0.2697, val_acc: 0.9483\n",
      "Epoch [59/200], time: 4.27s, loss: 0.2813, acc: 0.9420, val_loss: 0.2693, val_acc: 0.9447\n",
      "Epoch [60/200], time: 4.20s, loss: 0.2800, acc: 0.9420, val_loss: 0.2674, val_acc: 0.9467\n",
      "Epoch [61/200], time: 4.17s, loss: 0.2792, acc: 0.9421, val_loss: 0.2664, val_acc: 0.9490\n",
      "Epoch [62/200], time: 4.21s, loss: 0.2787, acc: 0.9423, val_loss: 0.2664, val_acc: 0.9483\n",
      "Epoch [63/200], time: 4.17s, loss: 0.2784, acc: 0.9425, val_loss: 0.2656, val_acc: 0.9465\n",
      "Epoch [64/200], time: 4.15s, loss: 0.2776, acc: 0.9421, val_loss: 0.2693, val_acc: 0.9444\n",
      "Epoch [65/200], time: 4.06s, loss: 0.2801, acc: 0.9409, val_loss: 0.3091, val_acc: 0.9221\n",
      "Epoch [66/200], time: 4.27s, loss: 0.2770, acc: 0.9426, val_loss: 0.2638, val_acc: 0.9483\n",
      "Epoch [67/200], time: 4.08s, loss: 0.2756, acc: 0.9426, val_loss: 0.2643, val_acc: 0.9469\n",
      "Epoch [68/200], time: 4.18s, loss: 0.2748, acc: 0.9428, val_loss: 0.2635, val_acc: 0.9482\n",
      "Epoch [69/200], time: 4.15s, loss: 0.2745, acc: 0.9426, val_loss: 0.2650, val_acc: 0.9469\n",
      "Epoch [70/200], time: 4.16s, loss: 0.2738, acc: 0.9423, val_loss: 0.2628, val_acc: 0.9481\n",
      "Epoch [71/200], time: 4.30s, loss: 0.2731, acc: 0.9426, val_loss: 0.2615, val_acc: 0.9471\n",
      "Epoch [72/200], time: 4.14s, loss: 0.2730, acc: 0.9426, val_loss: 0.2612, val_acc: 0.9471\n",
      "Epoch [73/200], time: 4.13s, loss: 0.2746, acc: 0.9411, val_loss: 0.2627, val_acc: 0.9491\n",
      "Epoch [74/200], time: 4.18s, loss: 0.2716, acc: 0.9429, val_loss: 0.2602, val_acc: 0.9491\n",
      "Epoch [75/200], time: 4.17s, loss: 0.2736, acc: 0.9416, val_loss: 0.2636, val_acc: 0.9488\n",
      "Epoch [76/200], time: 4.18s, loss: 0.2709, acc: 0.9430, val_loss: 0.2606, val_acc: 0.9461\n",
      "Epoch [77/200], time: 3.82s, loss: 0.2704, acc: 0.9432, val_loss: 0.2596, val_acc: 0.9465\n",
      "Epoch [78/200], time: 4.15s, loss: 0.2707, acc: 0.9432, val_loss: 0.2651, val_acc: 0.9433\n",
      "Epoch [79/200], time: 4.12s, loss: 0.2692, acc: 0.9435, val_loss: 0.2590, val_acc: 0.9489\n",
      "Epoch [80/200], time: 4.18s, loss: 0.2763, acc: 0.9419, val_loss: 0.2815, val_acc: 0.9418\n",
      "Epoch [81/200], time: 4.10s, loss: 0.2690, acc: 0.9432, val_loss: 0.2571, val_acc: 0.9489\n",
      "Epoch [82/200], time: 4.10s, loss: 0.2702, acc: 0.9423, val_loss: 0.2611, val_acc: 0.9489\n",
      "Epoch [83/200], time: 4.07s, loss: 0.2678, acc: 0.9437, val_loss: 0.2599, val_acc: 0.9453\n",
      "Epoch [84/200], time: 4.30s, loss: 0.2672, acc: 0.9436, val_loss: 0.2568, val_acc: 0.9491\n",
      "Epoch [85/200], time: 4.19s, loss: 0.2667, acc: 0.9437, val_loss: 0.2565, val_acc: 0.9488\n",
      "Epoch [86/200], time: 4.24s, loss: 0.2664, acc: 0.9436, val_loss: 0.2563, val_acc: 0.9477\n",
      "Epoch [87/200], time: 3.98s, loss: 0.2682, acc: 0.9421, val_loss: 0.2583, val_acc: 0.9483\n",
      "Epoch [88/200], time: 3.72s, loss: 0.2655, acc: 0.9440, val_loss: 0.2555, val_acc: 0.9488\n",
      "Epoch [89/200], time: 4.04s, loss: 0.2653, acc: 0.9435, val_loss: 0.2550, val_acc: 0.9482\n",
      "Epoch [90/200], time: 4.17s, loss: 0.2655, acc: 0.9437, val_loss: 0.2600, val_acc: 0.9444\n",
      "Epoch [91/200], time: 4.07s, loss: 0.2646, acc: 0.9434, val_loss: 0.2545, val_acc: 0.9486\n",
      "Epoch [92/200], time: 4.25s, loss: 0.2648, acc: 0.9438, val_loss: 0.2595, val_acc: 0.9446\n",
      "Epoch [93/200], time: 4.31s, loss: 0.2637, acc: 0.9438, val_loss: 0.2526, val_acc: 0.9489\n",
      "Epoch [94/200], time: 4.31s, loss: 0.2629, acc: 0.9440, val_loss: 0.2531, val_acc: 0.9500\n",
      "Epoch [95/200], time: 4.24s, loss: 0.2633, acc: 0.9439, val_loss: 0.2538, val_acc: 0.9481\n",
      "Epoch [96/200], time: 4.15s, loss: 0.2624, acc: 0.9438, val_loss: 0.2536, val_acc: 0.9482\n",
      "Epoch [97/200], time: 4.14s, loss: 0.2672, acc: 0.9427, val_loss: 0.2703, val_acc: 0.9427\n",
      "Epoch [98/200], time: 3.82s, loss: 0.2622, acc: 0.9441, val_loss: 0.2535, val_acc: 0.9475\n",
      "Epoch [99/200], time: 4.38s, loss: 0.2616, acc: 0.9437, val_loss: 0.2525, val_acc: 0.9481\n",
      "Epoch [100/200], time: 3.97s, loss: 0.2610, acc: 0.9439, val_loss: 0.2517, val_acc: 0.9476\n",
      "Epoch [101/200], time: 4.03s, loss: 0.2608, acc: 0.9439, val_loss: 0.2501, val_acc: 0.9491\n",
      "Epoch [102/200], time: 4.12s, loss: 0.2603, acc: 0.9442, val_loss: 0.2512, val_acc: 0.9473\n",
      "Epoch [103/200], time: 4.16s, loss: 0.2602, acc: 0.9440, val_loss: 0.2499, val_acc: 0.9487\n",
      "Epoch [104/200], time: 4.03s, loss: 0.2599, acc: 0.9442, val_loss: 0.2500, val_acc: 0.9488\n",
      "Epoch [105/200], time: 4.14s, loss: 0.2597, acc: 0.9440, val_loss: 0.2505, val_acc: 0.9474\n",
      "Epoch [106/200], time: 4.19s, loss: 0.2591, acc: 0.9444, val_loss: 0.2493, val_acc: 0.9489\n",
      "Epoch [107/200], time: 4.04s, loss: 0.2593, acc: 0.9443, val_loss: 0.2490, val_acc: 0.9478\n",
      "Epoch [108/200], time: 4.24s, loss: 0.2586, acc: 0.9444, val_loss: 0.2486, val_acc: 0.9485\n",
      "Epoch [109/200], time: 3.88s, loss: 0.2579, acc: 0.9442, val_loss: 0.2503, val_acc: 0.9474\n",
      "Epoch [110/200], time: 4.08s, loss: 0.2603, acc: 0.9429, val_loss: 0.2506, val_acc: 0.9493\n",
      "Epoch [111/200], time: 4.38s, loss: 0.2578, acc: 0.9444, val_loss: 0.2479, val_acc: 0.9482\n",
      "Epoch [112/200], time: 4.15s, loss: 0.2591, acc: 0.9432, val_loss: 0.2494, val_acc: 0.9491\n",
      "Epoch [113/200], time: 4.14s, loss: 0.2572, acc: 0.9447, val_loss: 0.2477, val_acc: 0.9488\n",
      "Epoch [114/200], time: 4.18s, loss: 0.2571, acc: 0.9445, val_loss: 0.2466, val_acc: 0.9482\n",
      "Epoch [115/200], time: 4.06s, loss: 0.2571, acc: 0.9445, val_loss: 0.2475, val_acc: 0.9489\n",
      "Epoch [116/200], time: 4.15s, loss: 0.2564, acc: 0.9449, val_loss: 0.2468, val_acc: 0.9480\n",
      "Epoch [117/200], time: 4.23s, loss: 0.2560, acc: 0.9448, val_loss: 0.2482, val_acc: 0.9479\n",
      "Epoch [118/200], time: 4.01s, loss: 0.2560, acc: 0.9446, val_loss: 0.2472, val_acc: 0.9480\n",
      "Epoch [119/200], time: 4.06s, loss: 0.2555, acc: 0.9448, val_loss: 0.2458, val_acc: 0.9486\n",
      "Epoch [120/200], time: 4.06s, loss: 0.2553, acc: 0.9446, val_loss: 0.2457, val_acc: 0.9486\n",
      "Epoch [121/200], time: 4.12s, loss: 0.2550, acc: 0.9448, val_loss: 0.2465, val_acc: 0.9481\n",
      "Epoch [122/200], time: 4.15s, loss: 0.2548, acc: 0.9449, val_loss: 0.2476, val_acc: 0.9489\n",
      "Epoch [123/200], time: 4.21s, loss: 0.2544, acc: 0.9448, val_loss: 0.2468, val_acc: 0.9491\n",
      "Epoch [124/200], time: 4.25s, loss: 0.2541, acc: 0.9446, val_loss: 0.2464, val_acc: 0.9490\n",
      "Epoch [125/200], time: 4.21s, loss: 0.2539, acc: 0.9448, val_loss: 0.2453, val_acc: 0.9480\n",
      "Epoch [126/200], time: 4.19s, loss: 0.2540, acc: 0.9448, val_loss: 0.2478, val_acc: 0.9473\n",
      "Epoch [127/200], time: 4.36s, loss: 0.2535, acc: 0.9449, val_loss: 0.2450, val_acc: 0.9481\n",
      "Epoch [128/200], time: 4.11s, loss: 0.2537, acc: 0.9448, val_loss: 0.2459, val_acc: 0.9480\n",
      "Epoch [129/200], time: 3.99s, loss: 0.2640, acc: 0.9437, val_loss: 0.2491, val_acc: 0.9476\n",
      "Epoch [130/200], time: 4.31s, loss: 0.2527, acc: 0.9449, val_loss: 0.2431, val_acc: 0.9491\n",
      "Epoch [131/200], time: 4.05s, loss: 0.2526, acc: 0.9450, val_loss: 0.2439, val_acc: 0.9486\n",
      "Epoch [132/200], time: 4.04s, loss: 0.2532, acc: 0.9433, val_loss: 0.2476, val_acc: 0.9495\n",
      "Epoch [133/200], time: 4.00s, loss: 0.2523, acc: 0.9451, val_loss: 0.2451, val_acc: 0.9479\n",
      "Epoch [134/200], time: 4.30s, loss: 0.2519, acc: 0.9448, val_loss: 0.2430, val_acc: 0.9485\n",
      "Epoch [135/200], time: 4.23s, loss: 0.2517, acc: 0.9449, val_loss: 0.2428, val_acc: 0.9483\n",
      "Epoch [136/200], time: 4.14s, loss: 0.2514, acc: 0.9449, val_loss: 0.2438, val_acc: 0.9480\n",
      "Epoch [137/200], time: 4.43s, loss: 0.2531, acc: 0.9438, val_loss: 0.2444, val_acc: 0.9486\n",
      "Epoch [138/200], time: 4.24s, loss: 0.2510, acc: 0.9451, val_loss: 0.2421, val_acc: 0.9487\n",
      "Epoch [139/200], time: 3.74s, loss: 0.2577, acc: 0.9438, val_loss: 0.2760, val_acc: 0.9436\n",
      "Epoch [140/200], time: 4.17s, loss: 0.2510, acc: 0.9451, val_loss: 0.2421, val_acc: 0.9489\n",
      "Epoch [141/200], time: 4.17s, loss: 0.2501, acc: 0.9453, val_loss: 0.2440, val_acc: 0.9489\n",
      "Epoch [142/200], time: 4.36s, loss: 0.2502, acc: 0.9453, val_loss: 0.2420, val_acc: 0.9491\n",
      "Epoch [143/200], time: 4.27s, loss: 0.2501, acc: 0.9451, val_loss: 0.2429, val_acc: 0.9486\n",
      "Epoch [144/200], time: 4.29s, loss: 0.2496, acc: 0.9451, val_loss: 0.2416, val_acc: 0.9486\n",
      "Epoch [145/200], time: 4.25s, loss: 0.2495, acc: 0.9453, val_loss: 0.2419, val_acc: 0.9487\n",
      "Epoch [146/200], time: 4.05s, loss: 0.2521, acc: 0.9439, val_loss: 0.2432, val_acc: 0.9485\n",
      "Epoch [147/200], time: 4.12s, loss: 0.2511, acc: 0.9440, val_loss: 0.2443, val_acc: 0.9492\n",
      "Epoch [148/200], time: 4.05s, loss: 0.2486, acc: 0.9454, val_loss: 0.2414, val_acc: 0.9499\n",
      "Epoch [149/200], time: 3.80s, loss: 0.2497, acc: 0.9436, val_loss: 0.2482, val_acc: 0.9482\n",
      "Epoch [150/200], time: 4.42s, loss: 0.2485, acc: 0.9454, val_loss: 0.2410, val_acc: 0.9486\n",
      "Epoch [151/200], time: 4.34s, loss: 0.2486, acc: 0.9451, val_loss: 0.2419, val_acc: 0.9491\n",
      "Epoch [152/200], time: 4.15s, loss: 0.2477, acc: 0.9458, val_loss: 0.2402, val_acc: 0.9491\n",
      "Epoch [153/200], time: 4.18s, loss: 0.2499, acc: 0.9438, val_loss: 0.2472, val_acc: 0.9486\n",
      "Epoch [154/200], time: 4.26s, loss: 0.2480, acc: 0.9456, val_loss: 0.2401, val_acc: 0.9487\n",
      "Epoch [155/200], time: 4.15s, loss: 0.2472, acc: 0.9456, val_loss: 0.2414, val_acc: 0.9488\n",
      "Epoch [156/200], time: 4.10s, loss: 0.2469, acc: 0.9457, val_loss: 0.2414, val_acc: 0.9482\n",
      "Epoch [157/200], time: 4.13s, loss: 0.2468, acc: 0.9458, val_loss: 0.2402, val_acc: 0.9503\n",
      "Epoch [158/200], time: 4.27s, loss: 0.2470, acc: 0.9457, val_loss: 0.2409, val_acc: 0.9497\n",
      "Epoch [159/200], time: 3.36s, loss: 0.2466, acc: 0.9460, val_loss: 0.2399, val_acc: 0.9497\n",
      "Epoch [160/200], time: 4.20s, loss: 0.2480, acc: 0.9449, val_loss: 0.2414, val_acc: 0.9499\n",
      "Epoch [161/200], time: 4.20s, loss: 0.2466, acc: 0.9460, val_loss: 0.2401, val_acc: 0.9498\n",
      "Epoch [162/200], time: 4.11s, loss: 0.2461, acc: 0.9464, val_loss: 0.2384, val_acc: 0.9506\n",
      "Epoch [163/200], time: 4.14s, loss: 0.2458, acc: 0.9460, val_loss: 0.2386, val_acc: 0.9507\n",
      "Epoch [164/200], time: 4.15s, loss: 0.2477, acc: 0.9449, val_loss: 0.2438, val_acc: 0.9502\n",
      "Epoch [165/200], time: 4.30s, loss: 0.2455, acc: 0.9461, val_loss: 0.2397, val_acc: 0.9500\n",
      "Epoch [166/200], time: 4.22s, loss: 0.2452, acc: 0.9462, val_loss: 0.2396, val_acc: 0.9504\n",
      "Epoch [167/200], time: 4.08s, loss: 0.2455, acc: 0.9462, val_loss: 0.2387, val_acc: 0.9500\n",
      "Epoch [168/200], time: 4.35s, loss: 0.2450, acc: 0.9465, val_loss: 0.2399, val_acc: 0.9508\n",
      "Epoch [169/200], time: 3.76s, loss: 0.2447, acc: 0.9465, val_loss: 0.2385, val_acc: 0.9507\n",
      "Epoch [170/200], time: 4.07s, loss: 0.2448, acc: 0.9464, val_loss: 0.2382, val_acc: 0.9512\n",
      "Epoch [171/200], time: 4.09s, loss: 0.2440, acc: 0.9461, val_loss: 0.2415, val_acc: 0.9500\n",
      "Epoch [172/200], time: 4.06s, loss: 0.2442, acc: 0.9462, val_loss: 0.2381, val_acc: 0.9506\n",
      "Epoch [173/200], time: 4.33s, loss: 0.2440, acc: 0.9466, val_loss: 0.2381, val_acc: 0.9504\n",
      "Epoch [174/200], time: 4.25s, loss: 0.2441, acc: 0.9461, val_loss: 0.2423, val_acc: 0.9491\n",
      "Epoch [175/200], time: 4.17s, loss: 0.2435, acc: 0.9464, val_loss: 0.2383, val_acc: 0.9508\n",
      "Epoch [176/200], time: 4.14s, loss: 0.2435, acc: 0.9462, val_loss: 0.2383, val_acc: 0.9507\n",
      "Epoch [177/200], time: 4.27s, loss: 0.2432, acc: 0.9464, val_loss: 0.2385, val_acc: 0.9506\n",
      "Epoch [178/200], time: 4.26s, loss: 0.2456, acc: 0.9452, val_loss: 0.2411, val_acc: 0.9511\n",
      "Epoch [179/200], time: 3.73s, loss: 0.2431, acc: 0.9467, val_loss: 0.2369, val_acc: 0.9513\n",
      "Epoch [180/200], time: 4.07s, loss: 0.2427, acc: 0.9464, val_loss: 0.2371, val_acc: 0.9508\n",
      "Epoch [181/200], time: 4.12s, loss: 0.2425, acc: 0.9465, val_loss: 0.2385, val_acc: 0.9508\n",
      "Epoch [182/200], time: 4.18s, loss: 0.2425, acc: 0.9465, val_loss: 0.2369, val_acc: 0.9505\n",
      "Epoch [183/200], time: 4.18s, loss: 0.2445, acc: 0.9453, val_loss: 0.2391, val_acc: 0.9512\n",
      "Epoch [184/200], time: 4.10s, loss: 0.2422, acc: 0.9465, val_loss: 0.2366, val_acc: 0.9508\n",
      "Epoch [185/200], time: 4.24s, loss: 0.2419, acc: 0.9468, val_loss: 0.2360, val_acc: 0.9509\n",
      "Epoch [186/200], time: 4.08s, loss: 0.2416, acc: 0.9466, val_loss: 0.2376, val_acc: 0.9509\n",
      "Epoch [187/200], time: 4.21s, loss: 0.2416, acc: 0.9467, val_loss: 0.2365, val_acc: 0.9508\n",
      "Epoch [188/200], time: 4.21s, loss: 0.2418, acc: 0.9464, val_loss: 0.2363, val_acc: 0.9508\n",
      "Epoch [189/200], time: 3.83s, loss: 0.2413, acc: 0.9466, val_loss: 0.2391, val_acc: 0.9504\n",
      "Epoch [190/200], time: 4.21s, loss: 0.2412, acc: 0.9465, val_loss: 0.2373, val_acc: 0.9508\n",
      "Epoch [191/200], time: 4.39s, loss: 0.2439, acc: 0.9455, val_loss: 0.2372, val_acc: 0.9513\n",
      "Epoch [192/200], time: 4.01s, loss: 0.2407, acc: 0.9465, val_loss: 0.2360, val_acc: 0.9512\n",
      "Epoch [193/200], time: 4.32s, loss: 0.2404, acc: 0.9464, val_loss: 0.2371, val_acc: 0.9512\n",
      "Epoch [194/200], time: 4.30s, loss: 0.2403, acc: 0.9465, val_loss: 0.2357, val_acc: 0.9508\n",
      "Epoch [195/200], time: 4.17s, loss: 0.2403, acc: 0.9466, val_loss: 0.2350, val_acc: 0.9508\n",
      "Epoch [196/200], time: 4.21s, loss: 0.2402, acc: 0.9466, val_loss: 0.2363, val_acc: 0.9512\n",
      "Epoch [197/200], time: 4.04s, loss: 0.2400, acc: 0.9468, val_loss: 0.2368, val_acc: 0.9506\n",
      "Epoch [198/200], time: 4.20s, loss: 0.2567, acc: 0.9455, val_loss: 0.2433, val_acc: 0.9489\n",
      "Epoch [199/200], time: 3.74s, loss: 0.2399, acc: 0.9465, val_loss: 0.2372, val_acc: 0.9508\n",
      "Epoch [200/200], time: 4.07s, loss: 0.2395, acc: 0.9466, val_loss: 0.2352, val_acc: 0.9515\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "history = train(translator, optimizer, train_loader, valid_loader, epochs=epochs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ad07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 模型翻译测试\n",
    "max_len = 40\n",
    "test = 'what is it'\n",
    "test = test.split(' ')\n",
    "eng = []\n",
    "for t in test:\n",
    "    eng.append(eng2idx[t])\n",
    "    \n",
    "eng = eng + [0]*(max_len - len(eng))\n",
    "eng = torch.IntTensor(np.array([eng], dtype=int))\n",
    "eng = Variable(eng).cuda()\n",
    "predict = [chs2idx['B']]*(max_len - 1)\n",
    "predict = np.array([int(i.cpu()) if type(i)!=int else i for i in predict])\n",
    "chs = ''\n",
    "\n",
    "for i in range(max_len - 2):\n",
    "    encoder_state = translator.initHidden(eng, 1)\n",
    "    pre = torch.IntTensor(np.array([predict], dtype=int))\n",
    "    pre = Variable(pre).cuda()\n",
    "    output = translator(pre, encoder_state)\n",
    "    # 提取最大概率的字符所在的位置，记录其编号\n",
    "    index = torch.argmax(output) \n",
    "    predict[i+1] = index\n",
    "    \n",
    "    if predict[i+1] == 0:\n",
    "        break\n",
    "        \n",
    "    # 提取上述编号所对应的字符\n",
    "    current_word = [k for k, v in chs2idx.items() if v==index][0] \n",
    "    chs = chs + current_word\n",
    "    \n",
    "print(chs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cops3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
